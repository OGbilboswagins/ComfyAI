æˆ‘éœ€è¦å‚è€ƒå¦å¤–ä¸€ä¸ªå·¥ç¨‹çš„ä»£ç ï¼Œæ¥å®ç°æˆ‘å½“å‰çš„åç«¯èƒ½åŠ›ï¼Œå½“å‰çš„å·¥ç¨‹æ˜¯ä¸ªComfyUIçš„æ’ä»¶ï¼Œæˆ‘åœ¨è¿™é‡Œå®ç°äº†ä¸€ä¸ªmcp_client,æ¥æ›¿ä»£æ‰åŸæ¥çš„workflowChatApi.tsé‡Œçš„`${BASE_URL}/api/chat/invoke`æ¥å£ã€‚

æ ¸å¿ƒä»£ç facade.pyå®ç°äº†agent-toolsçš„èƒ½åŠ›ï¼š
```python
from typing import Optional, Generator, Dict
import json
import uuid
from agent.tools.node_explain_agent_tool import explain_node
from agent.tools.node_search_tool import search_node
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.postgres import PostgresSaver
from context import get_user_id
from logger import logger
from psycopg_pool import ConnectionPool
from agent.llms import llm_4_chatbot, llm_4o, llm_deepseek, llm_4o_mini, llm_dashscope, get_openai_client, llm_dashscope_deepseek_v3, llm_qwen3, llm_4_1, llm_gemini2_5_flash
from agent.tools.workflow_recall_tool import recall_workflow
from agent.tools.gen_workflow_tool import gen_workflow
from agent.tools.model_search_tool import search_model
from initializer.db_initializer import Session, db_url
from utils.os_util import get_root_path
from dao.message_dao import Message, MessageSource
from dao.node_dao import Node
from dao.workflow_dao import Workflow

tools = [
    recall_workflow,  # å·¥ä½œæµå¬å›
    gen_workflow,  # å·¥ä½œæµç”Ÿæˆ
    search_node,  # èŠ‚ç‚¹æœç´¢
    search_model,  # æ¨¡å‹å¬å›
    explain_node,  # èŠ‚ç‚¹è§£é‡Š
    # analyze_workflow,  # å·¥ä½œæµåˆ†æ
]
system_prompt_path = get_root_path('prompts/system_prompt.txt')
with open(system_prompt_path, 'r', encoding='utf-8') as f:
    system_prompt_content = f.read()
new_system_prompt = SystemMessage(content=system_prompt_content)

connection_kwargs = {
    "autocommit": True,
    "prepare_threshold": 0,
}

# with ConnectionPool(
#     conninfo=db_url,
#     max_size=20,
#     kwargs=connection_kwargs,
# ) as pool:
pool = ConnectionPool(db_url, max_size=50, kwargs=connection_kwargs)
checkpointer = PostgresSaver(pool)
checkpointer.setup()

# checkpointer = MemorySaver()

# ä¿®æ”¹ LLM æ˜ å°„å­—å…¸ï¼Œä½¿ç”¨æ­£ç¡®çš„é…ç½®æ–¹å¼
llm_config: Dict = {
    "gemini-2.5-flash": {
        "client":llm_gemini2_5_flash,
        "image_enable":True,
    },
    "gpt-4o": {
        "client":llm_4_1,
        "image_enable":True,
    },
    "gpt-4.1": {
        "client":llm_4_1,
        "image_enable":True,
    },
    "qwen-plus": {
        "client":llm_dashscope,
        "image_enable":False,
    },
}

llm_mapping = {k: llm_config[k]["client"] for k in llm_config}


# è·å–æ‰€æœ‰llmåˆ—è¡¨
def get_model_list():
    model_list = []
    remove_models = ["gpt-4o"]
    for model, config in llm_config.items():
        if model not in remove_models:
            model_list.append({
                "name": model,
                "image_enable": config["image_enable"],
            })
    return model_list

# åˆ›å»ºä¸€ä¸ªå‡½æ•°ç”¨äºé™åˆ¶å¯¹è¯å†å²
def limit_conversation_history(state, max_turns=5):
    """
    é™åˆ¶å¯¹è¯å†å²åªä¿ç•™æœ€è¿‘çš„å‡ è½®å¯¹è¯
    
    Args:
        state: å½“å‰çš„çŠ¶æ€å¯¹è±¡
        max_turns: ä¿ç•™çš„æœ€å¤§å¯¹è¯è½®æ•°
    
    Returns:
        ä¿®æ”¹åçš„çŠ¶æ€
    """
    messages = state.get("messages", [])
    system_messages = []
    conversation_messages = []
    
    # åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œå¯¹è¯æ¶ˆæ¯
    for msg in messages:
        if isinstance(msg, SystemMessage):
            system_messages.append(msg)
        else:
            conversation_messages.append(msg)
    
    # ä¿ç•™å¯¹è¯æ¶ˆæ¯çš„æœ€åmax_turns*2æ¡(æ¯è½®åŒ…å«ç”¨æˆ·æ¶ˆæ¯å’ŒAIå›å¤)
    limited_conversation = conversation_messages[-max_turns*2:] if len(conversation_messages) > max_turns*2 else conversation_messages
    
    # é‡æ–°ç»„åˆæ¶ˆæ¯
    state["messages"] = system_messages + limited_conversation
    return state

# å°†ç³»ç»Ÿæç¤ºå’Œå†å²é™åˆ¶ç»„åˆåœ¨ä¸€èµ·
def combined_state_modifier(state):
    """
    ç»„åˆçŠ¶æ€ä¿®æ”¹å™¨ï¼Œæ·»åŠ ç³»ç»Ÿæç¤ºå¹¶é™åˆ¶å¯¹è¯å†å²
    
    Args:
        state: å½“å‰çš„çŠ¶æ€å¯¹è±¡
    
    Returns:
        ä¿®æ”¹åçš„æ¶ˆæ¯åˆ—è¡¨
    """
    messages = state.get("messages", [])
    
    # åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œå¯¹è¯æ¶ˆæ¯
    system_messages = []
    conversation_messages = []
    
    for msg in messages:
        if isinstance(msg, SystemMessage):
            system_messages.append(msg)
        else:
            conversation_messages.append(msg)
    
    # å¦‚æœæ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ·»åŠ ç³»ç»Ÿæç¤º
    if not system_messages and isinstance(new_system_prompt, SystemMessage):
        system_messages = [new_system_prompt]
    
    # ä¿ç•™å¯¹è¯æ¶ˆæ¯çš„æœ€å5è½®(æ¯è½®åŒ…å«ç”¨æˆ·æ¶ˆæ¯å’ŒAIå›å¤)
    max_turns = 10
    limited_conversation = conversation_messages[-max_turns*2:] if len(conversation_messages) > max_turns*2 else conversation_messages
    
    # è¿”å›ç³»ç»Ÿæ¶ˆæ¯å’Œé™åˆ¶åçš„å¯¹è¯æ¶ˆæ¯çš„ç»„åˆ
    return system_messages + limited_conversation

# åˆ›å»ºåŸºç¡€ agent æ—¶ä½¿ç”¨ combined_state_modifier
agent_executor = create_react_agent(
    llm_4o,
    tools,
    checkpointer=checkpointer,
    state_modifier=combined_state_modifier,
)


def is_return_direct_tool(tool_name: str) -> bool:
    return any([tool_name == t.name and t.return_direct for t in tools])


def is_json_str(value: str) -> bool:
    try:
        json.loads(value)
        return True
    except ValueError:
        return False


def chat(
        query: str,
        config: Optional[RunnableConfig] = None,
        session_id: str=None,
        debug: bool = False,
) -> (Generator[tuple[str, any], None, None]):
    logger.info(f"agent query: {query}")

    assert session_id is not None, "session_id is required"
    common_info = {
        "user_id": get_user_id() or -1,
        "session_id": session_id,
        "sub_session_id": str(uuid.uuid4()).replace('-', ''),
    }
    
    model = config.get("model") if config else None

    with Session() as session:
        Message.create(session, **common_info, source=MessageSource.USER.value, content=query, attributes={"model": model})

    if config and config.get("images"):
        images = config["images"]
        content = [{"type": "text", "text": query}]
        for image in images:
            content.append({
                "type": "image_url",
                "image_url": {"url": image.data}
            })
        agent_input = {"messages": [HumanMessage(content=content)]}
    else:
        agent_input = {"messages": [HumanMessage(content=query)]}

    # æ£€æŸ¥è‡ªå®šä¹‰ OpenAI å‡­æ®
    custom_api_key = config.get("openai_api_key") if config else None
    custom_base_url = config.get("openai_base_url") if config else None

    # åˆ›å»º LLM æ˜ å°„
    if custom_api_key:
        # ä½¿ç”¨å®¢æˆ·ç«¯ç®¡ç†å™¨è·å–å®¢æˆ·ç«¯
        custom_llm_mapping = {
            "gpt-4o": get_openai_client("gpt-4o", custom_api_key, custom_base_url),
            # "gpt-4o-mini": get_openai_client("gpt-4o-mini", custom_api_key, custom_base_url),
        }
        # å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤å®ä¾‹
        custom_llm_mapping.update({
            "DeepSeek-V3": llm_deepseek,
            "qwen-plus": llm_dashscope
        })
    else:
        custom_llm_mapping = llm_mapping

    # æ ¹æ® config ä¸­çš„ model å‚æ•°é€‰æ‹© LLM
    if config and config.get("model") and config["model"] in custom_llm_mapping:
        selected_llm = custom_llm_mapping[config["model"]]
        agent = create_react_agent(selected_llm, tools, checkpointer=checkpointer, state_modifier=combined_state_modifier)
    else:
        agent = agent_executor
    agent.step_timeout = 60  # å°†å•æ­¥è¶…æ—¶æ—¶é—´å¢åŠ åˆ°60ç§’

    try:
        agent_response = agent.stream(agent_input, config, stream_mode="messages")

        current_text = ''
        # data = None
        ext = None
        tool_results = {}  # å­˜å‚¨ä¸åŒå·¥å…·çš„ç»“æœ
        workflow_tools_called = set()  # è·Ÿè¸ªè°ƒç”¨çš„å·¥ä½œæµå·¥å…·
        
        for chunk, _ in agent_response:
            content = chunk.content
            chunk_type = chunk.type
            if not content or not chunk_type:
                continue

            if chunk_type == 'tool':
                tool_name = chunk.name
                attributes = {
                    "tool_name": tool_name,
                    "model": model,
                }
                with Session() as session:
                    Message.create(session, **common_info, source=MessageSource.TOOL.value, content=content, attributes=attributes)
            
                if is_json_str(content):
                    content_dict = json.loads(content)
                    answer = content_dict.get("answer")
                    data = content_dict.get("data")
                    tool_ext = content_dict.get("ext")
                    
                    # å­˜å‚¨å·¥å…·ç»“æœ
                    tool_results[tool_name] = {
                        "answer": answer,
                        "data": data,
                        "ext": tool_ext,
                        "content_dict": content_dict
                    }
                    
                    # è·Ÿè¸ªå·¥ä½œæµç›¸å…³å·¥å…·çš„è°ƒç”¨
                    if tool_name in ["recall_workflow", "gen_workflow"]:
                        workflow_tools_called.add(tool_name)
                        
                else:
                    answer = content
                    data = None
                    tool_ext = None
                    tool_results[tool_name] = {
                        "answer": answer,
                        "data": data,
                        "ext": tool_ext,
                        "content_dict": None
                    }
                    
                    # è·Ÿè¸ªå·¥ä½œæµç›¸å…³å·¥å…·çš„è°ƒç”¨
                    if tool_name in ["recall_workflow", "gen_workflow"]:
                        workflow_tools_called.add(tool_name)

                if is_return_direct_tool(chunk.name):
                    logger.info(f"agent response answer: {answer}")
                    if tool_ext:
                        logger.info(f"agent response ext: {tool_ext}")
                    yield answer, tool_ext
                    return
            elif chunk_type == 'AIMessageChunk':
                current_text += content
                if not debug:
                    yield current_text, None

        # å¤„ç†å·¥ä½œæµå·¥å…·çš„ç»“æœåˆå¹¶
        workflow_tools_found = [tool for tool in ["recall_workflow", "gen_workflow"] if tool in tool_results]
        
        if workflow_tools_found:
            logger.info(f"Workflow tools called: {workflow_tools_found}")
            
            # æ£€æŸ¥æ˜¯å¦åŒæ—¶è°ƒç”¨äº†ä¸¤ä¸ªå·¥ä½œæµå·¥å…·
            if "recall_workflow" in tool_results and "gen_workflow" in tool_results:
                logger.info("Both recall_workflow and gen_workflow were called, merging results")
                
                # æ£€æŸ¥æ¯ä¸ªå·¥å…·æ˜¯å¦æˆåŠŸæ‰§è¡Œ
                successful_workflows = []
                
                recall_result = tool_results["recall_workflow"]
                if recall_result["data"] and len(recall_result["data"]) > 0:
                    successful_workflows.extend(recall_result["data"])
                    logger.info(f"recall_workflow succeeded with {len(recall_result['data'])} workflows")
                else:
                    logger.info("recall_workflow failed or returned no data")
                
                gen_result = tool_results["gen_workflow"]
                if gen_result["data"] and len(gen_result["data"]) > 0:
                    successful_workflows.extend(gen_result["data"])
                    logger.info(f"gen_workflow succeeded with {len(gen_result['data'])} workflows")
                else:
                    logger.info("gen_workflow failed or returned no data")
                
                # åˆ›å»ºæœ€ç»ˆçš„ext
                if successful_workflows:
                    ext = [{
                        "type": "workflow",
                        "data": successful_workflows
                    }]
                    logger.info(f"Returning {len(successful_workflows)} workflows from successful tools")
                else:
                    ext = None
                    logger.info("No successful workflow data to return")
                    
            elif "recall_workflow" in tool_results and "gen_workflow" not in tool_results:
                # åªè°ƒç”¨äº†recall_workflowï¼Œä¸è¿”å›extï¼Œä¿æŒfinished=false
                logger.info("Only recall_workflow was called, waiting for gen_workflow, not returning ext")
                ext = None
                
            elif "gen_workflow" in tool_results and "recall_workflow" not in tool_results:
                # åªè°ƒç”¨äº†gen_workflowçš„æƒ…å†µï¼Œæ­£å¸¸è¿”å›ç»“æœ
                logger.info("Only gen_workflow was called, returning its result")
                gen_result = tool_results["gen_workflow"]
                if gen_result["data"] and len(gen_result["data"]) > 0:
                    ext = [{
                        "type": "workflow",
                        "data": gen_result["data"]
                    }]
                    logger.info(f"Returning {len(gen_result['data'])} workflows from gen_workflow")
                else:
                    ext = None
                    logger.info("gen_workflow failed or returned no data")
        else:
            # æ²¡æœ‰è°ƒç”¨å·¥ä½œæµå·¥å…·ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å·¥å…·è¿”å›äº†ext
            for tool_name, result in tool_results.items():
                if result["ext"]:
                    ext = result["ext"]
                    logger.info(f"Using ext from {tool_name}")
                    break

        with Session() as session:
            filtered_images = []
            if config and config.get("images"):
                filtered_images = [{k: v for k, v in img.dict().items() if k != 'data'} for img in config["images"]] if config["images"] else []
            Message.create(session, **common_info, source=MessageSource.AI.value, content=current_text, attributes={"ext": ext, "images": filtered_images, "model": model})

        logger.info(f"agent response answer: {current_text}")
        if ext:
            logger.info(f"agent response ext: {ext}")
        yield current_text, ext
    except TimeoutError:
        logger.error("Agent execution timed out", exc_info=True)
        yield "I apologize, but the request timed out. Please try again or rephrase your question.", None
        return
    except Exception as e:
        logger.error(f"Agent execution failed: {str(e)}", exc_info=True)
        yield f"I apologize, but an error occurred: {str(e)}", None
        return
```

æ¥å£è°ƒç”¨ä¾§ä»£ç å¦‚ä¸‹ï¼š
```python

router = APIRouter()
# conversations: dict[str, any] = {}


@router.post('/invoke')
def invoke_post(request: ChatRequest, req: Request):
    return _handle_invoke(request, req)


@router.get('/invoke')
def invoke_get(req: Request):
    request = ChatRequest(**req.query_params)
    return _handle_invoke(request, req)


def _handle_invoke(request: ChatRequest, req: Request):
    # Extract headers
    accept_language = req.headers.get('Accept-Language')
    openai_base_url = req.headers.get('Openai-Base-Url')
    encrypted_api_key = req.headers.get('Encrypted-Openai-Api-Key')
    model = req.headers.get('Model')
    user_type = getattr(req.state, 'user_type', None)
    
    if user_type == "new":
        def new_user_response_generator():
            if accept_language == "zh-CN":
                text = """
æ‚¨å¥½ï¼æˆ‘æ˜¯ComfyUI-Copilotï¼Œæ‚¨çš„AIå·¥ä½œæµåŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®æ‚¨å¿«é€Ÿä¸Šæ‰‹ComfyUIã€æé«˜å·¥ä½œæµè°ƒä¼˜æ•ˆç‡ï¼ŒåŒ…æ‹¬ï¼š

- åˆ›å»ºComfyUIå·¥ä½œæµã€æä¾›å·¥ä½œæµæ¨¡æ¿å’Œç¤ºä¾‹ï¼›
- æä¾›èŠ‚ç‚¹ä½¿ç”¨å»ºè®®å’Œæœ€ä½³å®è·µ
- è§£ç­”ComfyUIç›¸å…³é—®é¢˜
- å¸®åŠ©è°ƒè¯•å’Œä¼˜åŒ–ç°æœ‰å·¥ä½œæµ
- æŸ¥è¯¢ç›¸å…³æ¨¡å‹ä¿¡æ¯ç­‰

è¯·å‘Šè¯‰æˆ‘ä½ éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ååŠ©ä½ å®ŒæˆComfyUIç›¸å…³ä»»åŠ¡ã€‚
ç¬¬ä¸€æ¬¡è®¿é—®æˆ‘ä»¬çš„æœåŠ¡è¯·è®¿é—®[é“¾æ¥](https://form.typeform.com/to/tkg91K8D)å®ŒæˆKeyç”³è¯·(å…è´¹)ï¼Œå¹¶ç‚¹å‡»âš™ï¸é…ç½®Keyã€‚
                """
            else:
                text = """Hello! I'm ComfyUI-Copilot, your AI workflow assistant. I can help you quickly get started with ComfyUI and improve your workflow optimization efficiency, including:

- Creating ComfyUI workflows and providing workflow templates and examples
- Offering node usage recommendations and best practices
- Answering ComfyUI-related questions
- Helping debug and optimize existing workflows
- Querying relevant model information

Please let me know what assistance you need, and I'll do my best to help you with your ComfyUI-related tasks.

For first-time visitors, please visit [link](https://form.typeform.com/to/tkg91K8D) to complete your Key application (free), and click âš™ï¸ to configure your Key.
                """
            
            response = ChatResponse(
                session_id=request.session_id,
                text=text,
                format="markdown",
                finished=True
            )
            yield response.model_dump_json()
            yield '\n'
        
        return StreamingResponse(new_user_response_generator())
    
    if user_type == "unregister":
        def unregister_response_generator():
            if accept_language == "zh-CN":
                text = "ComfyUI-CopilotæœåŠ¡å‡çº§ï¼Œè¯·è®¿é—®[é“¾æ¥](https://form.typeform.com/to/tkg91K8D)è¾“å…¥é‚®ç®±å®ŒæˆKeyç”³è¯·(å…è´¹)ï¼Œå¹¶ç‚¹å‡»âš™ï¸é…ç½®Keyã€‚è‹¥åŸå…ˆå·²æœ‰Keyï¼Œéœ€è¦ä¿®æ”¹æˆé‚®ç®±æ–°æ”¶åˆ°çš„Keyã€‚"
            else:
                text = "ComfyUI-Copilot service upgrade, please visit [link](https://form.typeform.com/to/tkg91K8D) to input your email and complete your Key application (free), and click ğŸ”¨ to configure your Key. If you already have a Key, you need to modify it to the new Key."
            
            response = ChatResponse(
                session_id=request.session_id,
                text=text,
                format="markdown",
                finished=True
            )
            yield response.model_dump_json()
            yield '\n'
        
        return StreamingResponse(unregister_response_generator())
    
    # Build config
    config = {}
    if accept_language:
        config["language"] = accept_language
    if openai_base_url:
        config["openai_base_url"] = openai_base_url
    
    # Handle encrypted API key if provided
    if encrypted_api_key:
        try:
            openai_api_key = crypto_util.decrypt_with_private_key(encrypted_api_key)
            logger.info("Successfully decrypted OpenAI API key")
            config["openai_api_key"] = openai_api_key
        except Exception as e:
            logger.error(f"Failed to decrypt OpenAI API key: {str(e)}")
            # Continue with potentially None openai_api_key
    
    if model:
        config["model"] = model
        
    return do_invoke(request, config)
```