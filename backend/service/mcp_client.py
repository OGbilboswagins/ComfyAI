# Patched version – MCP fully disabled but Copilot fully functional
# Generated by ChatGPT for stability with local openai-compatible LLMs

from ..utils.globals import BACKEND_BASE_URL, get_comfyui_copilot_api_key
from .. import core
import asyncio
import traceback
from typing import List, Dict, Any

from ..agent_factory import create_agent
from ..service.workflow_rewrite_agent import create_workflow_rewrite_agent
from ..utils.request_context import get_rewrite_context, get_session_id, get_config
from ..utils.logger import log

from openai.types.responses import ResponseTextDeltaEvent
from openai import APIError, RateLimitError
from pydantic import BaseModel


# -------------------------------------------------------------
#  IMPORTANT:
#  MCP IS FULLY DISABLED IN THIS PATCH.
#  The original code attempted to start TWO MCP SSE servers,
#  including one with a dead URL, causing TaskGroup failures.
#
#  This patched version:
#   - Completely removes MCPServerSse creation
#   - Provides stub "server_list" to keep Agent constructor happy
#   - Bypasses async MCP server context entirely
#   - Still allows full LLM + workflow tool support
# -------------------------------------------------------------


class ImageData:
    """Image data structure"""
    def __init__(self, filename: str, data: str, url: str = None):
        self.filename = filename
        self.data = data
        self.url = url


async def comfyui_agent_invoke(messages: List[Dict[str, Any]], images: List[ImageData] = None):
    """
    Patched: MCP disabled. Uses direct LLM invocation.
    """
    try:
        # Context
        session_id = get_session_id()
        config = get_config()

        if not session_id:
            raise ValueError("No session_id found in request context")
        if not config:
            raise ValueError("No config found in request context")

        # MCP servers replaced with stubs
        log.info("[MCP DISABLED] MCP servers are bypassed in patched version.")

        class DummyServer:
            async def __aenter__(self): return self
            async def __aexit__(self, exc_type, exc, tb): return False

        mcp_server = DummyServer()
        bing_server = DummyServer()
        server_list = []   # Empty MCP server list – required by agent constructor

        # ---- Workflow rewrite agent hook ----
        workflow_rewrite_agent_instance = create_workflow_rewrite_agent()

        class HandoffRewriteData(BaseModel):
            rewrite_intent: str

        async def on_handoff(ctx, input_data: HandoffRewriteData):
            get_rewrite_context().rewrite_intent = input_data.rewrite_intent
            log.info(f"Rewrite agent called with intent: {input_data.rewrite_intent}")

        # Construct main agent
        from agents import handoff
        handoff_rewrite = handoff(
            agent=workflow_rewrite_agent_instance,
            input_type=HandoffRewriteData,
            on_handoff=on_handoff,
        )

        agent = create_agent(
            name="ComfyUI-Copilot",
            instructions=config.get("instructions", ""),
            mcp_servers=server_list,
            handoffs=[handoff_rewrite],
            config=config,
        )

        # Input already in OpenAI format
        agent_input = messages
        log.info(f"-- Processing {len(messages)} messages")

        # Run LLM in streamed mode
        from agents.run import Runner
        result = Runner.run_streamed(agent, input=agent_input, max_turns=30)

        current_text = ""
        last_yield_length = 0
        tool_results = {}
        workflow_update_ext = None

        async def process_stream(stream_result):
            nonlocal current_text, last_yield_length, workflow_update_ext

            async for event in stream_result.stream_events():
                # TEXT DELTA
                if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                    delta = event.data.delta
                    if delta:
                        current_text += delta
                        if len(current_text) > last_yield_length:
                            last_yield_length = len(current_text)
                            yield (current_text, None)
                    continue

                # TOOL OUTPUT
                if event.type == "run_item_stream_event" and event.item.type == "tool_call_output_item":
                    try:
                        import json
                        output = json.loads(str(event.item.output))
                        tool_results["last"] = output
                        # Capture ext if exists
                        ext = output.get("ext")
                        if ext:
                            workflow_update_ext = ext
                    except:
                        pass
                    continue

        # Process the stream
        async for part in process_stream(result):
            yield part

        # Final ext block
        ext_final = {
            "data": workflow_update_ext,
            "finished": True
        }
        yield (current_text, ext_final)

    except Exception as e:
        log.error(f"Patched comfyui_agent_invoke error: {e}")
        log.error(traceback.format_exc())

        # Provide clean error response
        error_ext = {"data": None, "finished": True}
        yield (f"An error occurred: {e}", error_ext)
