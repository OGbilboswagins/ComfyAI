[
    {
        "name": "æ·»åŠ LoRA",
        "description": "åœ¨ç°æœ‰å·¥ä½œæµä¸­æ·»åŠ LoRAèŠ‚ç‚¹",
        "content": "åœ¨ç°æœ‰å·¥ä½œæµä¸­æ·»åŠ LoRAèŠ‚ç‚¹ï¼Œç¡®ä¿ä¸ç°æœ‰æ¨¡å‹å’Œæç¤ºè¯èŠ‚ç‚¹æ­£ç¡®è¿æ¥\n    åœ¨checkpointèŠ‚ç‚¹åæ·»åŠ LoRAèŠ‚ç‚¹ã€‚\n    {\n      \"1\": {\n         \"inputs\": {\n            \"lora_name\": \"DOG.safetensors\",\n            \"strength_model\": 1,\n            \"strength_clip\": 1,\n            \"model\": [\n            \"2\",\n            0\n            ],\n            \"clip\": [\n            \"2\",\n            1\n            ]\n         },\n         \"class_type\": \"LoraLoader\",\n         \"_meta\": {\n            \"title\": \"Load LoRA\"\n         }\n      }"
    },
    {
        "name": "åå¤„ç†å¢å¼º",
        "description": "åå¤„ç†å¢å¼ºï¼Œä¾‹å¦‚åœ¨Preview Imageæˆ–Save ImageèŠ‚ç‚¹åæ·»åŠ é«˜æ¸…æ”¾å¤§åŠŸèƒ½ï¼ˆå¦‚Real-ESRGANã€ESRGANç­‰ï¼‰; æˆ–è€…æ·»åŠ å›¾åƒç¼©æ”¾èŠ‚ç‚¹",
        "content": "- åœ¨Preview Imageæˆ–Save ImageèŠ‚ç‚¹åæ·»åŠ é«˜æ¸…æ”¾å¤§åŠŸèƒ½ï¼ˆå¦‚Real-ESRGANã€ESRGANç­‰ï¼‰\n       - æ·»åŠ å›¾åƒç¼©æ”¾èŠ‚ç‚¹\n       {\n  \"1\": {\n    \"inputs\": {\n      \"width\": 512,\n      \"height\": 512,\n      \"interpolation\": \"nearest\",\n      \"method\": \"stretch\",\n      \"condition\": \"always\",\n      \"multiple_of\": 0\n    },\n    \"class_type\": \"ImageResize+\",\n    \"_meta\": {\n      \"title\": \"ğŸ”§ Image Resize\"\n    }\n  }\n}\n       - æ·»åŠ å›¾åƒå°ºå¯¸è°ƒæ•´èŠ‚ç‚¹\n       {\n  \"2\": {\n    \"inputs\": {\n      \"aspect_ratio\": \"original\",\n      \"proportional_width\": 2,\n      \"proportional_height\": 1,\n      \"fit\": \"letterbox\",\n      \"method\": \"lanczos\",\n      \"round_to_multiple\": \"8\",\n      \"scale_to_longest_side\": false,\n      \"longest_side\": 1024\n    },\n    \"class_type\": \"LayerUtility: ImageScaleByAspectRatio\",\n    \"_meta\": {\n      \"title\": \"LayerUtility: ImageScaleByAspectRatio\"\n    }\n  }\n}\n   -æ·»åŠ å›¾åƒæ”¾å¤§èŠ‚ç‚¹\n  \"11\": {\n    \"inputs\": {\n      \"width\": 512,\n      \"height\": 512,\n      \"upscale_method\": \"nearest-exact\",\n      \"keep_proportion\": false,\n      \"divisible_by\": 2,\n      \"crop\": \"disabled\",\n      \"image\": [\n        \"12\",\n        0\n      ]\n    },\n    \"class_type\": \"ImageResizeKJ\",\n    \"_meta\": {\n      \"title\": \"Resize Image\"\n    }\n  },\n   -æ·»åŠ VAEdecodeèŠ‚ç‚¹\n  \"12\": {\n    \"inputs\": {},\n    \"class_type\": \"VAEDecode\",\n    \"_meta\": {\n      \"title\": \"VAE Decode\"\n    }\n  }\n}"
    },
    {
        "name": "æç¤ºè¯ä¼˜åŒ–",
        "description": "ä¿®æ”¹ç°æœ‰æç¤ºè¯èŠ‚ç‚¹çš„å†…å®¹(æç¤ºè¯åº”è¯¥åœ¨(CLIP Text Encode Promptæˆ–Text _OèŠ‚ç‚¹å†…ç¼–è¾‘); æ·»åŠ å•ç‹¬çš„æç¤ºè¯è¾“å…¥èŠ‚ç‚¹",
        "content": "{\n  \"12\": {\n    \"inputs\": {\n      \"text\": [\n        \"13\",\n        0\n      ]\n    },\n    \"class_type\": \"CLIPTextEncode\",\n    \"_meta\": {\n      \"title\": \"CLIP Text Encode (Prompt)\"\n    }\n  },\n  \"13\": {\n    \"inputs\": {\n      \"delimiter\": \", \",\n      \"clean_whitespace\": \"true\",\n      \"text_a\": [\n        \"15\",\n        0\n      ],\n      \"text_b\": [\n        \"16\",\n        0\n      ]\n    },\n    \"class_type\": \"Text Concatenate\",\n    \"_meta\": {\n      \"title\": \"Text Concatenate\"\n    }\n  },\n  \"15\": {\n    \"inputs\": {\n      \"text\": \"\"\n    },\n    \"class_type\": \"Text _O\",\n    \"_meta\": {\n      \"title\": \"Text _O\"\n    }\n  },\n  \"16\": {\n    \"inputs\": {\n      \"text\": \"\"\n    },\n    \"class_type\": \"Text _O\",\n    \"_meta\": {\n      \"title\": \"Text _O\"\n    }\n  }\n}"
    },
    {
        "name": "å›¾åƒåæ¨",
        "description": "ç†è§£å›¾ç‰‡ä¿¡æ¯å¹¶ç”¨æ–‡å­—è¡¨è¾¾å‡ºæ¥",
        "content": "æ·»åŠ å›¾åƒåæ¨èŠ‚ç‚¹ï¼ˆå¦‚CLIP Interrogatorï¼‰\n{\n  \"11\": {\n    \"inputs\": {\n      \"prompt_mode\": \"fast\",\n      \"image_analysis\": \"off\"\n    },\n    \"class_type\": \"ClipInterrogator\",\n    \"_meta\": {\n      \"title\": \"Clip Interrogator â™¾ï¸Mixlab\"\n    }\n  }\n}\n       - æ·»åŠ æ›´å¤æ‚æˆ–è€…æ›´å¥½ç”¨çš„å›¾åƒåæ¨èŠ‚ç‚¹(åŠ è½½å›¾ç‰‡ä½¿ç”¨florence2è¿›è¡Œåæ¨)\n       {\n  \"6\": {\n    \"inputs\": {\n      \"image\": \"06.JPG\"\n    },\n    \"class_type\": \"LoadImage\",\n    \"_meta\": {\n      \"title\": \"$image.image!:The image to analyze, must be a url\"\n    }\n  },\n  \"10\": {\n    \"inputs\": {\n      \"model\": \"microsoft/Florence-2-large\",\n      \"precision\": \"fp16\",\n      \"attention\": \"sdpa\"\n    },\n    \"class_type\": \"DownloadAndLoadFlorence2Model\",\n    \"_meta\": {\n      \"title\": \"DownloadAndLoadFlorence2Model\"\n    }\n  },\n  \"11\": {\n    \"inputs\": {\n      \"text_input\": \"\",\n      \"task\": \"more_detailed_caption\",\n      \"fill_mask\": true,\n      \"keep_model_loaded\": false,\n      \"max_new_tokens\": 1024,\n      \"num_beams\": 3,\n      \"do_sample\": true,\n      \"output_mask_select\": \"\",\n      \"seed\": 1098631327477633,\n      \"image\": [\n        \"6\",\n        0\n      ],\n      \"florence2_model\": [\n        \"10\",\n        0\n      ]\n    },\n    \"class_type\": \"Florence2Run\",\n    \"_meta\": {\n      \"title\": \"Florence2Run\"\n    }\n  },\n  \"18\": {\n    \"inputs\": {\n      \"anything\": [\n        \"11\",\n        2\n      ]\n    },\n    \"class_type\": \"easy showAnything\",\n    \"_meta\": {\n      \"title\": \"Show Any\"\n    }\n  },\n  \"20\": {\n    \"inputs\": {\n      \"value\": \"Generate high-quality text descriptions from images using local Florence model.\\n    \\n    Main use cases:\\n    1. **Reverse image prompt generation**:\\n       - When users upload an image and want to get prompts for AI art generation\\n       - Analyzes visual elements, style, composition, etc. to generate prompts for Stable Diffusion, DALL-E, and other models\\n       - In this case, return the tool's raw output directly to users without any modification or summary\\n       - Users can directly use these prompts for image generation\\n    2. **Image content understanding**:\\n       - When users ask about specific content, objects, scenes, people, etc. in the image\\n       - Need to understand the semantic content of the image and answer users' specific questions\\n       - In this case, combine tool output with conversation context to give users contextually appropriate natural responses\\n       - Don't return raw output directly, but provide targeted replies based on understanding results\"\n    },\n    \"class_type\": \"PrimitiveStringMultiline\",\n    \"_meta\": {\n      \"title\": \"MCP\"\n    }\n  }\n}"
    },
    {
        "name": "æ·»åŠ ControlNet",
        "description": "åœ¨ä½¿ç”¨é£æ ¼è¿ç§»ä¿æŒä¸»ä½“ç»“æ„ä¸å˜ã€ä¿æŒä¸»ä½“å½¢çŠ¶ã€é‡æ–°ç»˜åˆ¶èƒŒæ™¯ã€æ§åˆ¶äººç‰©è§’è‰²å§¿æ€ã€ä¿æŒåœºæ™¯çš„æ·±åº¦å’Œç©ºé—´ç»“æ„ã€åŸºäºçº¿ç¨¿æˆ–è€…è‰å›¾ç”Ÿæˆå›¾åƒç­‰åœºæ™¯ä¸‹ï¼Œå¯ä»¥ç”¨æ·»åŠ ControlNetèŠ‚ç‚¹æ¥å®ç°",
        "content": "**ControlNeté›†æˆ**ï¼š(preprocessorå¯é€‰cannyã€depthç­‰å‚æ•°,æ¨¡å‹æ ·å¼é€‰æ‹©ï¼ŒControlnetå¼€å§‹ã€ç»“æŸæƒé‡)\n       {\n  \"22\": {\n    \"inputs\": {\n      \"strength\": 1,\n      \"start_percent\": 0,\n      \"end_percent\": 1,\n      \"control_net\": [\n        \"23\",\n        0\n      ],\n      \"image\": [\n        \"24\",\n        0\n      ]\n    },\n    \"class_type\": \"ControlNetApplyAdvanced\",\n    \"_meta\": {\n      \"title\": \"Apply ControlNet\"\n    }\n  },\n  \"23\": {\n    \"inputs\": {\n      \"control_net_name\": \"ControlNet-Standard-Lineart-for-SDXL.safetensors\"\n    },\n    \"class_type\": \"ControlNetLoader\",\n    \"_meta\": {\n      \"title\": \"Load ControlNet Model\"\n    }\n  },\n  \"24\": {\n    \"inputs\": {\n      \"preprocessor\": \"none\",\n      \"resolution\": 512\n    },\n    \"class_type\": \"AIO_Preprocessor\",\n    \"_meta\": {\n      \"title\": \"AIO Aux Preprocessor\"\n    }\n  }\n}"
    },
    {
        "name": "æ‰©å›¾",
        "description": "ç»™æˆ‘ä¸€ä¸ªæ‰©å›¾é“¾è·¯(åŒ…æ‹¬å›¾åƒåŠ è½½å’Œå›¾åƒè¾“å‡º)",
        "content": "{\n  \"1\": {\n    \"inputs\": {\n      \"image\": \"01.JPG\"\n    },\n    \"class_type\": \"LoadImage\",\n    \"_meta\": {\n      \"title\": \"$image.image!:The image to analyze, must be a url\"\n    }\n  },\n  \"2\": {\n    \"inputs\": {\n      \"model\": \"runwayml/stable-diffusion-xl-base-1.0\",\n      \"precision\": \"fp16\",\n      \"attention\": \"sdpa\"\n    },\n    \"class_type\": \"DownloadAndLoadModel\",\n    \"_meta\": {\n      \"title\": \"DownloadAndLoadModel\"\n    }\n  },\n  \"3\": {\n    \"inputs\": {\n      \"prompt\": \"A beautiful girl with long hair and a white dress\",\n      \"image\": [\n        \"1\",\n        0\n      ],\n      \"model\": [\n        \"2\",\n        0\n      ]\n    },\n    \"class_type\": \"StableDiffusionXLRun\",\n    \"_meta\": {\n      \"title\": \"StableDiffusionXLRun\"\n    }\n  },\n  \"4\": {\n    \"inputs\": {\n      \"image\": [\n        \"3\",\n        0\n      ]\n    },\n    \"class_type\": \"easy showImage\",\n    \"_meta\": {\n      \"title\": \"Show Image\"\n    }\n  }\n}"
    },
    {
        "name": "æ™ºèƒ½æŠ å›¾",
        "description": "æ·»åŠ èƒŒæ™¯ç§»é™¤èŠ‚ç‚¹ï¼ˆå¦‚SAMã€UÂ²-Netç­‰ï¼‰",
        "content": "- æ·»åŠ èƒŒæ™¯ç§»é™¤æˆ–æŠ å›¾èŠ‚ç‚¹\n       {\n  \"7\": {\n    \"inputs\": {\n      \"rem_mode\": \"RMBG-1.4\",\n      \"image_output\": \"Preview\",\n      \"save_prefix\": \"ComfyUI\",\n      \"torchscript_jit\": false,\n      \"add_background\": \"none\",\n      \"refine_foreground\": false\n    },\n    \"class_type\": \"easy imageRemBg\",\n    \"_meta\": {\n      \"title\": \"Image Remove Bg\"\n    }\n  }\n}\n       - æ·»åŠ SAMæŠ å›¾èŠ‚ç‚¹\n       {\n  \"8\": {\n    \"inputs\": {\n      \"prompt\": \"\",\n      \"threshold\": 0.3,\n      \"sam_model\": [\n        \"9\",\n        0\n      ],\n      \"grounding_dino_model\": [\n        \"10\",\n        0\n      ]\n    },\n    \"class_type\": \"GroundingDinoSAMSegment (segment anything)\",\n    \"_meta\": {\n      \"title\": \"GroundingDinoSAMSegment (segment anything)\"\n    }\n  },\n  \"9\": {\n    \"inputs\": {\n      \"model_name\": \"sam_vit_h (2.56GB)\"\n    },\n    \"class_type\": \"SAMModelLoader (segment anything)\",\n    \"_meta\": {\n      \"title\": \"SAMModelLoader (segment anything)\"\n    }\n  },\n  \"10\": {\n    \"inputs\": {\n      \"model_name\": \"GroundingDINO_SwinT_OGC (694MB)\"\n    },\n    \"class_type\": \"GroundingDinoModelLoader (segment anything)\",\n    \"_meta\": {\n      \"title\": \"GroundingDinoModelLoader (segment anything)\"\n    }\n  }\n}"
    },
    {
        "name": "kontextå›¾åƒç¼–è¾‘",
        "description": "æ·»åŠ kontextç³»åˆ—èŠ‚ç‚¹(è¿™ä¸ªç³»åˆ—æ˜¯æ–‡ç”Ÿå›¾å’Œå›¾ç”Ÿå›¾åœºæ™¯åº”ç”¨çš„æ¨¡å‹å’ŒèŠ‚ç‚¹,é€‚åˆç”¨é’ˆå¯¹å›¾åƒçš„èåˆï¼ŒäºŒæ¬¡ç¼–è¾‘,åŒ…æ‹¬ä½†ä¸é™äºå»æ°´å°ã€æ“¦é™¤ç‰©ä½“ã€å¤šå…ƒç´ èåˆã€å›¾ç‰‡å…ƒç´ æå–ç­‰)",
        "content": "{\n  \"6\": {\n    \"inputs\": {\n      \"text\": \"\",\n      \"clip\": [\n        \"38\",\n        0\n      ]\n    },\n    \"class_type\": \"CLIPTextEncode\",\n    \"_meta\": {\n      \"title\": \"CLIP Text Encode (Prompt)\"\n    }\n  },\n  \"8\": {\n    \"inputs\": {\n      \"samples\": [\n        \"31\",\n        0\n      ],\n      \"vae\": [\n        \"39\",\n        0\n      ]\n    },\n    \"class_type\": \"VAEDecode\",\n    \"_meta\": {\n      \"title\": \"VAE Decode\"\n    }\n  },\n  \"31\": {\n    \"inputs\": {\n      \"seed\": 584043043142251,\n      \"steps\": 30,\n      \"cfg\": 1.5,\n      \"sampler_name\": \"euler\",\n      \"scheduler\": \"simple\",\n      \"denoise\": 1,\n      \"model\": [\n        \"37\",\n        0\n      ],\n      \"positive\": [\n        \"35\",\n        0\n      ],\n      \"negative\": [\n        \"135\",\n        0\n      ],\n      \"latent_image\": [\n        \"124\",\n        0\n      ]\n    },\n    \"class_type\": \"KSampler\",\n    \"_meta\": {\n      \"title\": \"KSampler\"\n    }\n  },\n  \"35\": {\n    \"inputs\": {\n      \"guidance\": 2.5,\n      \"conditioning\": [\n        \"177\",\n        0\n      ]\n    },\n    \"class_type\": \"FluxGuidance\",\n    \"_meta\": {\n      \"title\": \"FluxGuidance\"\n    }\n  },\n  \"37\": {\n    \"inputs\": {\n      \"unet_name\": \"flux1-dev-kontext_fp8_scaled.safetensors\",\n      \"weight_dtype\": \"fp8_e4m3fn\"\n    },\n    \"class_type\": \"UNETLoader\",\n    \"_meta\": {\n      \"title\": \"Load Diffusion Model\"\n    }\n  },\n  \"38\": {\n    \"inputs\": {\n      \"clip_name1\": \"clip_l.safetensors\",\n      \"clip_name2\": \"t5xxl_fp16.safetensors\",\n      \"type\": \"flux\",\n      \"device\": \"default\"\n    },\n    \"class_type\": \"DualCLIPLoader\",\n    \"_meta\": {\n      \"title\": \"DualCLIPLoader\"\n    }\n  },\n  \"39\": {\n    \"inputs\": {\n      \"vae_name\": \"ae.safetensors\"\n    },\n    \"class_type\": \"VAELoader\",\n    \"_meta\": {\n      \"title\": \"Load VAE\"\n    }\n  },\n  \"42\": {\n    \"inputs\": {\n      \"image\": [\n        \"194\",\n        0\n      ]\n    },\n    \"class_type\": \"FluxKontextImageScale\",\n    \"_meta\": {\n      \"title\": \"FluxKontextImageScale\"\n    }\n  },\n  \"124\": {\n    \"inputs\": {\n      \"pixels\": [\n        \"42\",\n        0\n      ],\n      \"vae\": [\n        \"39\",\n        0\n      ]\n    },\n    \"class_type\": \"VAEEncode\",\n    \"_meta\": {\n      \"title\": \"VAE Encode\"\n    }\n  },\n  \"135\": {\n    \"inputs\": {\n      \"conditioning\": [\n        \"6\",\n        0\n      ]\n    },\n    \"class_type\": \"ConditioningZeroOut\",\n    \"_meta\": {\n      \"title\": \"ConditioningZeroOut\"\n    }\n  },\n  \"177\": {\n    \"inputs\": {\n      \"conditioning\": [\n        \"6\",\n        0\n      ],\n      \"latent\": [\n        \"124\",\n        0\n      ]\n    },\n    \"class_type\": \"ReferenceLatent\",\n    \"_meta\": {\n      \"title\": \"ReferenceLatent\"\n    }\n  },\n  \"193\": {\n    \"inputs\": {\n      \"filename_prefix\": \"ComfyUI\",\n      \"images\": [\n        \"8\",\n        0\n      ]\n    },\n    \"class_type\": \"SaveImage\",\n    \"_meta\": {\n      \"title\": \"Save Image\"\n    }\n  },\n  \"194\": {\n    \"inputs\": {\n      \"image\": \"021175655C8A2A86C831C96855F3EF23.png\"\n    },\n    \"class_type\": \"LoadImage\",\n    \"_meta\": {\n      \"title\": \"Load Image\"\n    }\n  }\n}"
    },
    {
        "name": "qwen imageå›¾åƒç”Ÿæˆ",
        "description": "åŸºäºQwen-Imageå¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ–‡ç”Ÿå›¾è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒä¸­è‹±æ–‡æç¤ºè¯ï¼Œæ“…é•¿ç”ŸæˆåŒ…å«å¤æ‚æ–‡æœ¬çš„å›¾åƒå†…å®¹ã€‚é€‚ç”¨äºæµ·æŠ¥è®¾è®¡ã€å¹¿å‘Šåˆ›æ„ç­‰éœ€è¦ç¨³å®šç”Ÿæˆæ–‡å­—å†…å®¹çš„åœºæ™¯",
        "content": "æ·»åŠ qwen imageæ¨¡å‹,ä½¿ç”¨qwen image æ–‡ç”Ÿå›¾ï¼Œåˆ›ä½œä¸€å¼ æµ·æŠ¥ï¼Œé€‚åˆç”Ÿæˆç¨³å®šçš„æ–‡å­—(å½“ç”¨æˆ·æåˆ°éœ€è¦ä½¿ç”¨qwen imageç³»åˆ—æ—¶ç»™å®ƒæä¾›å¦‚ä¸‹çš„èŠ‚ç‚¹å’Œæ¨¡å‹é€‰æ‹©ï¼šload Difussion ModelèŠ‚ç‚¹å¯¹åº”qwen_imageåå­—çš„æ¨¡å‹ï¼ŒLoad CLIPå¯¹åº”çš„clip_name:qwen_2.5_vl,typeï¼šqwen_image,Load VAEèŠ‚ç‚¹å¯¹åº”çš„æ¨¡å‹ä¸º:qwen_image_vae.safetensors)"
    }
]